{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef25373-1599-442f-bdbf-3c207538ae61",
   "metadata": {},
   "source": [
    "# Bayesian inference in [JAX](https://github.com/google/jax)\n",
    "Brett Morris\n",
    "\n",
    "## Install\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <strong>WARNING</strong>: It's possible that installing these dependencies in your usual working environment will break your other packages. Don't do that!\n",
    "</div>\n",
    "\n",
    "\n",
    "Please use `conda` or [`venv`](https://docs.python.org/3/library/venv.html) to create an isolated python environment for this tutorial.\n",
    "\n",
    "For conda:\n",
    "\n",
    "```bash\n",
    "conda create -n jax-demo python=3.12\n",
    "conda activate jax-demo\n",
    "```\n",
    "\n",
    "For venv:\n",
    "\n",
    "```bash\n",
    "python -m venv /path/to/new/virtual/environment\n",
    "source <environment_name>/bin/activate\n",
    "```\n",
    "\n",
    "\n",
    "#### Installing jax\n",
    "\n",
    "Try the following on your laptop (CPU): \n",
    "```bash\n",
    "python -m pip install --upgrade \"jax[cpu]\"\n",
    "```\n",
    "jax can run on GPUs and TPUs but requires specific builds for each architecture. Check out the [jax installation docs](https://github.com/google/jax#installation) for details.\n",
    "\n",
    "#### Other dependencies\n",
    "\n",
    "Other installations needed for this tutorial can be installed with: \n",
    "\n",
    "```bash\n",
    "python -m pip install numpy scipy matplotlib numpyro arviz corner ipywidgets\n",
    "```\n",
    "\n",
    "## Why jax?\n",
    "\n",
    "jax leverages [just-in-time code compilation](https://docs.jax.dev/en/latest/jit-compilation.html), with [automatic differentiation](https://docs.jax.dev/en/latest/automatic-differentiation.html), and [_accelerated linear algebra_](https://github.com/openxla/xla) with a _numpy-like API_ to calculate blazing fast, differentiable models. Let's break that down: \n",
    "\n",
    "* Automatic differentiation allows you to compute gradients of your mathematical models without explicitly deriving gradients for each function. These gradients can be used in gradient-based inference techniques like gradient descent optimization, or Hamiltonian Monte Carlo.\n",
    "* Accelerated linear algebra package is an optimizing compiler designed for machine learning. You write Python code and it gets just-in-time compiled for your computer architecture (CPU or GPU) at runtime.\n",
    "\n",
    "\n",
    "## Getting started with jax\n",
    "\n",
    "First, let's do a bunch of imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea3d13b-f874-4b51-89e0-c63c5d8019aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bare necessities:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# in this example, numpyro must be imported before jax \n",
    "import numpyro\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "from numpyro import distributions as dist\n",
    "\n",
    "# Set the number of cores to use on your machine for \n",
    "# parallel computing on your CPU:\n",
    "cpu_cores = 4\n",
    "numpyro.set_host_device_count(cpu_cores)\n",
    "\n",
    "# From jax, we'll import the numpy module as `jnp`:\n",
    "from jax import numpy as jnp\n",
    "\n",
    "# `grad` computes gradients!\n",
    "from jax import grad\n",
    "\n",
    "# this module produces random numbers in jax:\n",
    "from jax import random\n",
    "\n",
    "# arviz is a statistical toolkit for analyzing \n",
    "# bayesian inference models and their posteriors\n",
    "import arviz\n",
    "\n",
    "# corner makes corner plots\n",
    "from corner import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384ada16-29d7-4fa9-a293-946e0d574ee1",
   "metadata": {},
   "source": [
    "`jax` has been designed to mimic the methods and arguments you're already accustomed to in `numpy`. For many functions, you can simply use the `jax.numpy` module rather than ordinary `numpy` to handle array calculations in `jax`. \n",
    "\n",
    "In some instances, converting your numpy model implementations to jax implementations can be as simple as replacing `np` with `jnp` in your code. ðŸª„\n",
    "\n",
    "For example, here's how we can create an array of linearly spaced values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a960af5a-4279-431f-a2f3-acec8cfbeb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linearly spaced `Array` object, which behaves like a np.ndarray:\n",
    "array = jnp.linspace(-5, 5, 20_000)\n",
    "\n",
    "array[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0fae9-f262-41b3-baec-f18f2cf49ff6",
   "metadata": {},
   "source": [
    "Note that the above code created a `Array` object, which is not an ordinary numpy array. This is limited to data type `float32` by default. `DeviceArray` objects have the usual built-in methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ed652-b9a0-47ff-94c0-4f804608c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "array.mean(), array.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a69814-1708-46eb-b2e0-491b1243810c",
   "metadata": {},
   "source": [
    "### Creating a synthetic dataset \n",
    "Now let's create some synthetic data which we'll fit using jax: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f3432e-759b-4fbd-842f-5972c2da24c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "x = array\n",
    "\n",
    "# Set the parameters of the double-Gaussian \n",
    "# profile in our synthetic data\n",
    "amp0 = 2\n",
    "amp1 = 1.5\n",
    "x0 = -0.5\n",
    "x1 = 0.6\n",
    "s0 = 0.6\n",
    "s1 = 0.9\n",
    "yerr = 0.3\n",
    "\n",
    "y_first_term = (\n",
    "    amp0 * np.exp(-0.5 * (x - x0)**2 / s0**2)\n",
    ")\n",
    "\n",
    "y_second_term = (\n",
    "    amp1 * np.exp(-0.5 * (x - x1)**2 / s1**2)\n",
    ")\n",
    "\n",
    "y_noise = np.random.normal(scale=yerr, size=(len(x)))\n",
    "\n",
    "y = y_first_term + y_second_term + y_noise\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(x, y, ',', color='silver')\n",
    "plt.plot(x, y_first_term, label='first term')\n",
    "plt.plot(x, y_second_term, label='second term')\n",
    "plt.plot(x, y_first_term + y_second_term, label='first + second')\n",
    "plt.legend()\n",
    "ax.set(\n",
    "    xlabel='x', \n",
    "    ylabel='y'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4704ba30-c8e5-4420-b416-83263acecd40",
   "metadata": {},
   "source": [
    "### Fitting with numpy/scipy\n",
    "\n",
    "We could fit the observations $(x, y)$ with numpy and scipy like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8dfbec-27c8-486c-b988-fd805913b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_numpy(p, x):\n",
    "    \"\"\"Numpy implementation of a double-Gaussian profile\"\"\"\n",
    "    a0, x0, s0, a1, x1, s1 = p\n",
    "    return (\n",
    "        # gaussian 0\n",
    "        a0 * np.exp(-0.5 * (x - x0)**2 / s0**2) + \n",
    "\n",
    "        # gaussian 1\n",
    "        a1 * np.exp(-0.5 * (x - x1)**2 / s1**2)\n",
    "    )\n",
    "\n",
    "def chi2_numpy(p, x, y, yerr):\n",
    "    \"\"\"chi^2 function to minimize with scipy\"\"\"\n",
    "    return np.sum((model_numpy(p, x) - y)**2 / yerr**2)\n",
    "\n",
    "init_guess = np.float32([1.5, -0.2, 0.7, 1.5, 0.2, 0.7])\n",
    "\n",
    "from scipy.optimize import minimize as scipy_minimize\n",
    "bestp_numpy = scipy_minimize(chi2_numpy, init_guess, args=(x, y, yerr), method='BFGS').x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce73d6bb-670a-425a-b72d-ddf48b6e80f1",
   "metadata": {},
   "source": [
    "The scipy `minimize` function does optimization _without_ computing gradients. The best fit solutions are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206f52ee-35bd-497e-86d1-3df712de32ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestp_numpy   # best fit parameter solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59686e33-033b-4cc2-a71d-a9ba870e061f",
   "metadata": {},
   "source": [
    "Which look like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68221083-ffd1-4b91-8a61-bd1ad87ad9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, ',', color='silver')\n",
    "plt.plot(x, model_numpy(init_guess, x), 'purple', ls=':', label='init')\n",
    "plt.plot(x, model_numpy(bestp_numpy, x), 'dodgerblue', lw=2, label='numpy')\n",
    "plt.legend()\n",
    "plt.gca().set(xlabel='x', ylabel='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e6c2a5-d3c8-4702-94c8-6fed318363a4",
   "metadata": {},
   "source": [
    "The initial guess is shown above in blue, and the best-fit model using numpy/scipy is shown in red. You can see that the best-fit model doesn't fit the observations particularly well. Now let's implement the same thing in jax. \n",
    "\n",
    "### Fitting with jax\n",
    "\n",
    "Let's specify the model that we will fit to the data using the numpy module within jax. We'll also \"decorate\" it with the `jit` decorator, which will compile the function for us at runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8631f2-0a94-4c20-8423-272c1e70679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The just-in-time decorator:\n",
    "from jax import jit\n",
    "\n",
    "@jit\n",
    "def model_jax(p, x):     \n",
    "    \"\"\"\n",
    "    Jax implementation of the `model_numpy` function.\n",
    "    \n",
    "    The use of `jnp` in place of `np` is the only difference \n",
    "    from the numpy version.\n",
    "    \"\"\"\n",
    "    a0, x0, s0, a1, x1, s1 = p\n",
    "    return (\n",
    "        # first gaussian\n",
    "        a0 * jnp.exp(-0.5 * (x - x0)**2 / s0**2) + \n",
    "        \n",
    "        # second gaussian\n",
    "        a1 * jnp.exp(-0.5 * (x - x1)**2 / s1**2)\n",
    "    )\n",
    "@jit\n",
    "def chi2_jax(p, x, y, yerr):\n",
    "    \"\"\"chi^2 function written for minimization with jax\"\"\"\n",
    "    return jnp.sum((model_jax(p, x) - y)**2 / yerr**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699473a2-ba48-4f5e-bfc2-bbe79f556847",
   "metadata": {},
   "source": [
    "Now we import the minimize module from the `scipy.optimize` API within jax: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a251754d-4d35-4fff-bf6b-64d510234f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax has its own scipy module which uses autodiffed gradients\n",
    "from jax.scipy.optimize import minimize as jax_minimize\n",
    "\n",
    "bestp_jax = jax_minimize(chi2_jax, init_guess, args=(x, y, yerr), method='bfgs').x\n",
    "\n",
    "# print the best-fit parameters\n",
    "bestp_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55294c8c-5b40-4690-b840-b49ad450124b",
   "metadata": {},
   "source": [
    "In the above cell, we have used _gradient-based_ optimization with the [BFGS method](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm). Note that we didn't have to specify the gradient of our model with respect to each free parameter, that was done for us!\n",
    "\n",
    "Let's plot the best-fit model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db98d34-fad9-4d66-8b74-8b10a7a719c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, ',', color='silver')\n",
    "plt.plot(x, model_numpy(init_guess, x), 'purple', ls=':', label='init guess')\n",
    "plt.plot(x, model_numpy(bestp_numpy, x), 'dodgerblue', lw=2, label='numpy')\n",
    "plt.plot(x, model_jax(bestp_jax, x), 'r', lw=2, label='jax')\n",
    "plt.legend();\n",
    "plt.gca().set(xlabel='x', ylabel='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fea854-c982-4695-b9ed-b24f1ca6b550",
   "metadata": {},
   "source": [
    "In the figure above, the blue curve is the initial guess, the magenta dotted curve is the best fit with Powell's method via numpy/scipy, and the red dashed curve is the best-fit with jax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc141b-1209-4788-b6fa-8ef3298e37c1",
   "metadata": {},
   "source": [
    "### automatic differentiation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a92a23-92e8-4a0a-853c-108ce3d8e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "\n",
    "def model_jax_wrapper(a0, x0, s0, a1, x1, s1, x):\n",
    "    return model_jax([a0, x0, s0, a1, x1, s1], x)\n",
    "\n",
    "# `in_axes` tells `vmap` over which axes to \n",
    "# map input parameters. We only want to vmap \n",
    "# over the `x` variable, the others get None:\n",
    "in_axes = tuple(len(bestp_jax) * [None]) + (0,)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for n, name in enumerate('a0 x0 s0 a1 a2 a3'.split()):\n",
    "    # vmap vectorizes the operation over `x`:\n",
    "    d_param_d_x = vmap(\n",
    "        \n",
    "        # take the gradient with respect to each model parameter:\n",
    "        grad(model_jax_wrapper, argnums=n), \n",
    "        in_axes=in_axes\n",
    "\n",
    "    # evaluate (d f / d theta_i) at the best-fit values for \n",
    "    # each parameter theta_i:\n",
    "    )(*bestp_jax, x)\n",
    "    ls = '-' if name.endswith('0') else ':'\n",
    "    plt.plot(x, d_param_d_x, label=f\"d f / d {name}\", ls=ls)\n",
    "ax.set(\n",
    "    xlabel='x',\n",
    "    ylabel='d/dx'\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261553b9-3dd7-4613-926f-bc9724d69600",
   "metadata": {},
   "source": [
    "### Speed comparison\n",
    "\n",
    "Now let's check if there's any speed difference between the two implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7298964-6946-4ec1-ab56-02d58133de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Numpy only:')\n",
    "time_numpy = %timeit -n 100 -o model_numpy(init_guess, x)\n",
    "print('\\n\\njax:')\n",
    "time_jax = %timeit -n 100 -o model_jax(init_guess, x)\n",
    "\n",
    "print(f'\\n\\njax model evaluation is {time_numpy.average / time_jax.average :.1f}x faster\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117e3617-aecc-4417-afc8-7d91d66e4e7e",
   "metadata": {},
   "source": [
    "So not only is the jax model evaluation is faster, but the best-fit solution is closer to the true answer. Great work jax!\n",
    "\n",
    "### Posterior inference with jax/numpyro\n",
    "\n",
    "Now let's infer posterior distributions for the parameters using more complex inference methods, using _numpyro_. We will define a _model_ which specifies _distributions_ that represent each parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f449d-cc32-4a03-a325-cf7534610759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpyro_model():\n",
    "    \"\"\"\n",
    "    Define a model to sample with the No U-Turn Sampler (NUTS) via numpyro.\n",
    "    \n",
    "    The two Gaussians are defined by an amplitude, mean, and standard deviation.\n",
    "    \n",
    "    To find unique solutions for the two Gaussians, we put non-overlapping bounded \n",
    "    priors on the two amplitudes, but vary the means and stddev's with identical \n",
    "    uniform priors. \n",
    "    \"\"\"\n",
    "    # Uniform priors for amplitudes\n",
    "    a0, a1 = numpyro.sample(\n",
    "        'amp', dist.Uniform(low=1, high=3),\n",
    "        sample_shape=(2,)\n",
    "    )\n",
    "\n",
    "    # Uniform priors for the means\n",
    "    x0, x1 = numpyro.sample('x', dist.Uniform(low=-4, high=4), sample_shape=(2,))\n",
    "\n",
    "    # Uniform priors for the stddev's\n",
    "    s0, s1 = numpyro.sample(\n",
    "        'sigma', dist.Uniform(low=0.5, high=1.5), \n",
    "        sample_shape=(2,)\n",
    "    )\n",
    "\n",
    "    # save the model computed at each step\n",
    "    model = numpyro.deterministic('model', model_jax([a0, x0, s0, a1, x1, s1], x))\n",
    "\n",
    "    # Normally distributed likelihood\n",
    "    numpyro.sample(\n",
    "        \"obs\", dist.Normal(\n",
    "            loc=model, \n",
    "            scale=yerr\n",
    "        ), obs=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2506cc6-5845-4252-9590-bde40eb1096d",
   "metadata": {},
   "source": [
    "Now we use the No U-Turn Sampler for gradient-based inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5952332c-89a0-4700-8551-a4f80d437252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random numbers in jax are generated like this:\n",
    "rng_seed = 0\n",
    "rng_keys = random.split(\n",
    "    random.key(rng_seed), \n",
    "    int(cpu_cores)\n",
    ")\n",
    "\n",
    "# Define a sampler, using here the No U-Turn Sampler (NUTS)\n",
    "# with a dense mass matrix:\n",
    "sampler = NUTS(\n",
    "    numpyro_model, \n",
    ")\n",
    "\n",
    "# Monte Carlo sampling for a number of steps and parallel chains: \n",
    "mcmc = MCMC(\n",
    "    sampler, \n",
    "    num_warmup=1_000,\n",
    "    num_samples=1_000, \n",
    "    num_chains=cpu_cores,\n",
    "    progress_bar=False,\n",
    ")\n",
    "\n",
    "# Run the MCMC\n",
    "mcmc.run(rng_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbfd8d9-2889-4b48-a050-d62d25427c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arviz converts a numpyro MCMC object to an `InferenceData` object based on xarray:\n",
    "result = arviz.from_numpyro(mcmc)\n",
    "\n",
    "# these are the inputs to the synthetic double-gaussian profile (blue lines)\n",
    "truths = {'amp': [amp0, amp1], 'sigma': [s0, s1], 'x': [x0, x1]}#, 'delta_x': None, 'q1': None, 'q2': None}\n",
    "\n",
    "# make a corner plot\n",
    "corner(\n",
    "    result, \n",
    "    var_names=['amp', 'sigma', 'x'],\n",
    "    quiet=True, \n",
    "    truths=truths\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e6687c-ae73-4a88-bb29-2d8a9837fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_draws = result.posterior['model'].to_numpy()  # shape: (chains, steps, y data points)\n",
    "\n",
    "n_chains, n_samples, n_data_points = model_draws.shape\n",
    "\n",
    "plt.plot(x, y, ',', color='gray')\n",
    "for i in range(n_chains):\n",
    "    for j in np.random.randint(0, n_samples, size=10):\n",
    "        plt.plot(x, model_draws[i, j], color=f'C{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709ee23c-2a03-404b-a0c3-7c6bb8477631",
   "metadata": {},
   "source": [
    "Let's check the results, especially the convergence metric called the [Gelman-Rubin statistic](https://ui.adsabs.harvard.edu/abs/1992StaSc...7..457G/abstract), $\\hat{r}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a15dc78-0f02-43cc-abdf-563c237c1462",
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.summary(result, var_names=['~model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18e79ec-58e2-42d4-8a61-676eed5c41db",
   "metadata": {},
   "source": [
    "As chains approach convergence, $\\hat{r}$ approaches unity from above. Good convergence is roughly $\\hat{r} \\lesssim 1.01$.\n",
    "\n",
    "The bimodal distribution in the posteriors appears like poor convergence. We can see in the plots above that the bimodal distribution stems from the first and second gaussian terms switching indices. $x_0$ swaps with $x_1$, etc. \n",
    "\n",
    "#### Reparameterization\n",
    "\n",
    "Below, we avoid this bimodality in the posteriors by constraining $x_0 < x_1$, so no swapping can happen. We enforce this constraint by reparameterizing the sampling parameters. \n",
    "\n",
    "We follow the traingular sampling approach in [Turk 1990](https://doi.org/10.1016/B978-0-08-050753-8.50015-2) and [Kipping (2013)](https://ui.adsabs.harvard.edu/abs/2013MNRAS.435.2152K/abstract). Rather than sampling uniformly in $x_0, x_1$, we sample uniformly from two new parameters $q_0  \\sim [0, 1]$ and $q_1 \\sim [0, 1]$. The transformations below give us constrained samples with $x_0 < x_1$:\n",
    "\\begin{eqnarray}\n",
    "x_0 &=& \\Delta ~ \\sqrt{q_0} q_1 + x_{\\rm min},\\\\\n",
    "\\delta x &=& \\Delta \\left(1 - \\sqrt{q_0}\\right),~~ {\\rm and}\\\\\n",
    "x_1 &=& x_0 + \\delta x\n",
    "\\end{eqnarray}\n",
    "where the sampling bounds in $x$-space are $(x_{\\rm min}, x_{\\rm max})$, and $\\Delta = x_{\\rm max} -  x_{\\rm min}$ is the full range.\n",
    "\n",
    "Here's what that looks like in an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6d1928-182a-4498-9921-bcd3e05e1454",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = np.random.uniform(0, 1, size=1000)\n",
    "q2 = np.random.uniform(0, 1, size=1000)\n",
    "\n",
    "x_min = -4\n",
    "x_max = 4\n",
    "Delta = x_max - x_min\n",
    "demo_x0 = Delta * (np.sqrt(q1) * q2) + x_min\n",
    "demo_delta_x = Delta * (1 - np.sqrt(q1))\n",
    "\n",
    "plt.scatter(demo_x0, demo_x0 + demo_delta_x)\n",
    "plt.gca().set(xlabel='x0', ylabel='x1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109c68c-692d-4f0a-bfc2-f0e591797a2b",
   "metadata": {},
   "source": [
    "Now let's use this reparameterization in our numpyro model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd684e3-b41a-43a7-ba03-c2d21dd59799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpyro_model_reparam():\n",
    "    \"\"\"\n",
    "    Define a model to sample with the No U-Turn Sampler (NUTS) via numpyro.\n",
    "    \n",
    "    The two Gaussians are defined by an amplitude, mean, and standard deviation.\n",
    "    \n",
    "    To find unique solutions for the two Gaussians, we put non-overlapping bounded \n",
    "    priors on the two amplitudes, but vary the means and stddev's with identical \n",
    "    uniform priors. \n",
    "    \"\"\"\n",
    "    # Uniform priors for amplitudes\n",
    "    a0, a1 = numpyro.sample(\n",
    "        'amp', dist.Uniform(low=1, high=3), \n",
    "        sample_shape=(2,)\n",
    "    )\n",
    "    \n",
    "    # Non-overlapping uniform priors for Gaussian means\n",
    "    q1 = numpyro.sample('q1', dist.Uniform(low=0, high=1))\n",
    "    q2 = numpyro.sample('q2', dist.Uniform(low=0, high=1))\n",
    "    x_min = -4\n",
    "    x_max = 4\n",
    "    \n",
    "    # reparameterize in the style of Turk (1990), or \n",
    "    # Kipping (2013) Equations 13 & 14\n",
    "    Delta = x_max - x_min\n",
    "    x0 = numpyro.deterministic(\n",
    "        'x0',\n",
    "        Delta * (jnp.sqrt(q1) * q2) + x_min\n",
    "    )\n",
    "    delta_x = numpyro.deterministic(\n",
    "        'delta_x',\n",
    "        # reparameterize in the style of Turk (1990)\n",
    "        Delta * (1 - jnp.sqrt(q1))\n",
    "    )\n",
    "    x1 = numpyro.deterministic('x1', x0 + delta_x)\n",
    "    \n",
    "    # Uniform priors for the stddev's\n",
    "    s0, s1 = numpyro.sample(\n",
    "        'sigma', dist.Uniform(low=0.5, high=1.5), \n",
    "        sample_shape=(2,)\n",
    "    )\n",
    "\n",
    "    model = numpyro.deterministic('model', model_jax([a0, x0, s0, a1, x1, s1], x))\n",
    "    \n",
    "    # Normally distributed likelihood\n",
    "    numpyro.sample(\n",
    "        \"obs\", dist.Normal(\n",
    "            loc=model, \n",
    "            scale=yerr\n",
    "        ), obs=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8d60f0-0ccb-4b91-8180-01275f9f9142",
   "metadata": {},
   "source": [
    "The above cell defines the model. Now the cell below defines how to sample the model, and runs the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc0b59-2c12-422f-9dfb-77c3431e12f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random numbers in jax are generated like this:\n",
    "rng_seed = 0\n",
    "rng_keys = random.split(\n",
    "    random.PRNGKey(rng_seed), \n",
    "    cpu_cores\n",
    ")\n",
    "\n",
    "# Define a sampler, using here the No U-Turn Sampler (NUTS)\n",
    "# with a dense mass matrix:\n",
    "sampler = NUTS(\n",
    "    numpyro_model_reparam, \n",
    "    dense_mass=True\n",
    ")\n",
    "\n",
    "# Monte Carlo sampling for a number of steps and parallel chains: \n",
    "mcmc = MCMC(\n",
    "    sampler, \n",
    "    num_warmup=1_000, \n",
    "    num_samples=5_000, \n",
    "    num_chains=cpu_cores,\n",
    "    progress_bar=False,\n",
    ")\n",
    "\n",
    "# Run the MCMC\n",
    "mcmc.run(rng_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c9df9-63ad-4bcd-ac2c-acb1d974d16a",
   "metadata": {},
   "source": [
    "Wow, that was fast! Now let's visualize the posteriors using `arviz` and `corner`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efcfc2-d68d-4ed4-a782-8094e24a9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arviz converts a numpyro MCMC object to an `InferenceData` object based on xarray:\n",
    "result = arviz.from_numpyro(mcmc)\n",
    "\n",
    "# these are the inputs to the synthetic double-gaussian profile (blue lines)\n",
    "truths = {'amp': [amp0, amp1], 'sigma': [s0, s1], 'x0': x0, 'x1': x1}\n",
    "\n",
    "# make a corner plot\n",
    "corner(\n",
    "    result, \n",
    "    var_names=['amp', 'sigma', 'x0', 'x1'],#, 'beta'],\n",
    "    quiet=True, \n",
    "    truths=truths\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd46040b-cbf8-4ff6-bd65-d737308b425e",
   "metadata": {},
   "source": [
    "Note how all posterior distributions contain the \"true\" value and the chains have converged. \n",
    "\n",
    "Let's see how the samples look in the data space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b31be71-8e86-4532-ac02-578e4f52cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_draws = result.posterior['model'].to_numpy()  # shape: (chains, steps, y data points)\n",
    "\n",
    "n_chains, n_samples, n_data_points = model_draws.shape\n",
    "\n",
    "plt.plot(x, y, ',', color='gray')\n",
    "for i in range(n_chains):\n",
    "    for j in np.random.randint(0, n_samples, size=10):\n",
    "        plt.plot(x, model_draws[i, j], color=f'C{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ecff60-6f1b-45f9-a7aa-2efd0e4bd66f",
   "metadata": {},
   "source": [
    "Let's check convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04d457-3373-439d-94bc-7a12d1f7188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.summary(result, var_names=['~model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a54b4-4ad8-4e68-afe0-c110d0b10e23",
   "metadata": {},
   "source": [
    "We've accurately and robustly inferred the six parameters, in no time at all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f9a3a-023b-477e-b636-fc68f4203f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
